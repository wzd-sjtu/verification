%  导入相关的数据  这里是分割好的数据

%  可以看到卷积核把图像进行了一些简单的处理
%input_data=result;

% data1=img;
% data1(data1==255)=0;
% figure;
% imshow(data1);
% core=[1/9 1/9  1/9;
%       1/9  1/9 1/9;
%       1/9 1/9  1/9];
% tmp_data1=conv_c(data1,core);
% figure;
% imshow(tmp_data1);
clc 
clear
train_number=10000;
test_number=2000;

pic=imread('pic.png');

%%
%定义网络的各层参数

%  各层的数目
num_input=1;

num_c1=6;
num_s2=6;
num_c3=16;
num_s4=16;
num_c5=120;
num_connect=84;
num_output=10;%  这里的输出可以多上几层，从而对别的字符进行识别


%  各层操作时图像的尺寸以及卷积核尺寸
h_input=32;w_input=32;
h_c1=28;w_c1=28;
h_s2=14;w_s2=14;
h_c3=10;w_c3=10;
h_s4=5;w_s4=5;

h_c5=1;w_c5=1;%  第五层卷积过后仅仅出现一个神经元
h_connect=1;w_connect=1;
h_output=1;w_output=1;
h_conv=5;w_conv=5;
h_pool=2;w_pool=2;

%  各层神经元数目以及偏置

num_neure_input=32*32;
num_neure_c1=6*28*28;
num_neure_s2=6*14*14;
num_neure_c3=16*10*10;
num_neure_s4=16*5*5;
num_neure_c5=1*120;
num_neure_connect=1*84; %中间有一步全连接层，最后是输出层
num_neure_output=10;

%  以下是权值的尺寸
num_weight_c1=5*5*6;
num_weight_s2=6;
num_weight_c3=5*5*16*6;
num_weight_s4=16;
num_weight_c5=5*5*16*120;
num_weight_connect=120*84;
num_weight_output=84*10;

num_bias_c1=6;
num_bias_s2=6;
num_bias_c3=16;
num_bias_s4=16;
num_bias_c5=120;
num_bias_connect=84;
num_bias_output=10;

%  训练参数
max_generations=100;
accuracy=0.99;
learning_rate=0.01;
step=1e-8;  %步长or误差率

%%
%  网络各种矩阵初始化
%  这里的初始化使用了Xavier initialization方法进行初始化，防止出现梯度消失
%  初始化方法  m是输入数据维度，n是输出数据维度，产生在[-sqrt(6/(m+n)),sqrt(6/(m+n))]区间里面的均匀分布参数
%  可以在使用tanh作为激活函数时让数据不出现梯度消失的情况
%  不妨只考虑输入的情况
w_num_c1=reshape(2*(rand(1,num_weight_c1)-0.5)*sqrt(6/175),5,5,6);
%  这里是池化层的偏置
w_num_s2=2*(rand(1,num_weight_s2)-0.5)*(sqrt(6/5));
w_num_c3=2*(rand(1,num_weight_c3)-0.5)*(sqrt(6/550));w_num_c3=reshape(w_num_c3,5,5,6,16);
w_num_s4=2*(rand(1,num_weight_s4)-0.5)*(sqrt(6/5));
w_num_c5=2*(rand(1,num_weight_c5)-0.5)*(sqrt(6/3400));w_num_c5=reshape(w_num_c5,5,5,16,120);
w_num_connect=2*(rand(1,num_weight_connect)-0.5)*sqrt(6/100);w_num_connect=reshape(w_num_connect,120,84);
w_num_output=2*(rand(1,num_weight_output)-0.5)*(sqrt(6/100));w_num_output=reshape(w_num_output,84,10);
bias_c1=zeros(1,num_bias_c1);
bias_s2=zeros(1,num_bias_s2);
bias_c3=zeros(1,num_bias_c3);
bias_s4=zeros(1,num_bias_s4);
bias_c5=zeros(1,num_bias_c5);
bias_connect=zeros(1,num_bias_connect);
bias_output=zeros(1,num_bias_output);


delta_w_c1=zeros(1,num_weight_c1);delta_w_c1=reshape(delta_w_c1,5,5,6);%权值及偏置更新量
delta_w_s2=zeros(1,num_weight_s2);
delta_w_c3=zeros(1,num_weight_c3);delta_w_c3=reshape(delta_w_c3,5,5,6,16);
delta_w_s4=zeros(1,num_weight_s4);
delta_w_c5=zeros(1,num_weight_c5);delta_w_c5=reshape(delta_w_c5,5,5,16,120);
delta_w_connect=zeros(1,num_weight_connect);delta_w_connect=reshape(delta_w_connect,120,84);
delta_w_output=zeros(1,num_weight_output);delta_w_output=reshape(delta_w_output,84,10);

delta_bias_c1=zeros(1,num_bias_c1);
delta_bias_s2=zeros(1,num_bias_s2);
delta_bias_c3=zeros(1,num_bias_c3);
delta_bias_s4=zeros(1,num_bias_s4);
delta_bias_c5=zeros(1,num_bias_c5);
delta_bias_connect=zeros(1,num_bias_connect);
delta_bias_output=zeros(1,num_bias_output);

%  下面规定每层神经元的输出
neure_c1=zeros(h_c1,w_c1,num_c1);
neure_s2=zeros(h_s2,w_s2,num_s2);
neure_c3=zeros(h_c3,w_c3,num_c3);
neure_s4=zeros(h_s4,w_s4,num_s4);
neure_c5=zeros(h_c5,w_c5,num_c5);
neure_connect=zeros(h_connect,w_connect,num_connect);
neure_output=zeros(h_output,w_output,num_output);

delta_neure_c1=zeros(h_c1,w_c1,num_c1);
delta_neure_s2=zeros(h_s2,w_s2,num_s2);
delta_neure_c3=zeros(h_c3,w_c3,num_c3);
delta_neure_s4=zeros(h_s4,w_s4,num_s4);
delta_neure_c5=zeros(h_c5,w_c5,num_c5);
delta_neure_connect=zeros(h_connect,w_connect,num_connect);
delta_neure_output=zeros(h_output,w_output,num_output);


%% 
%  下面是矩阵的正向传播过程以及中间存储的参数

%  下面对矩阵进行正向传播变化
imshow('pic.png')
%%
% 输入层
img=double(rgb2gray(imread('pic.png')));
%  0是黑色  255是白色
% img(img~=255)=30;
% img(img==255)=0;

%c1卷积层
%  根据卷积核形成六个影像矩阵 这里的卷积核还没有进行变换，所以出现的结果是非常奇怪的
for i=1:num_c1
    neure_c1(:,:,i)=tanh(conv_c(img,w_num_c1(:,:,i)+bias_c1(i)));
    %figure;
    %imshow(neure_c1(:,:,i));
end

%%
%s2池化层
%  发现池化出来了一些比较奇怪的东西 
for i=1:num_s2
   neure_s2(:,:,i)=tanh(pooling(neure_c1(:,:,i),w_num_s2(i))+bias_s2(i));
   %figure;
   %imshow(neure_s2(:,:,i));
end

%%
% c3卷积层
%  下面进行适当的卷积操作

%  按照某种规则进行卷积操作
%  这个是训练矩阵
%  这里的取均值在初始化时已经完成了考虑emm
tablets=[1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1;
         1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1;
         1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1;
         0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1;
         0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1;
         0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1;
    ];
for j=1:num_c3
   for i=1:num_s2
      if tablets(i,j)==1
         neure_c3(:,:,j)=neure_c3(:,:,j)+conv_c(neure_s2(:,:,i),w_num_c3(:,:,i,j)); 
      end
   end
   neure_c3(:,:,j)=tanh(neure_c3(:,:,j)+bias_c3(j));
end

%%
%  s4池化层

for i=1:num_s4
    neure_s4(:,:,i)=tanh(pooling(neure_c3(:,:,i),w_num_s4(i))+bias_s4(i));
end

%%
%  C5层
%  每一个神经元和前面的所有层连接在一起。一共有120个神经元
for j=1:num_c5
   for i=1:num_s4
      neure_c5(:,:,j)=neure_c5(:,:,j)+conv_c(neure_s4(:,:,i),w_num_c5(:,:,i,j));
      
   end
   neure_c5(:,:,j)=tanh(neure_c5(:,:,j)+bias_c5(j));
end

%%
% 全连接层 connect
% 其实可以从120层直接连接到输出层的
tmp_neure_c5=reshape(neure_c5,1,120);
for i=1:num_connect
    neure_connect(:,:,i)=tmp_neure_c5*w_num_connect(:,i);
    neure_connect(:,:,i)=tanh(neure_connect(:,:,i)+bias_connect(i));
end

%%
%  输出层
%  暂时确定是10个输出尝试一下
tmp_neure_connect=reshape(neure_connect,1,84);
for i=1:num_output
   neure_output(:,:,i)=tmp_neure_connect*w_num_output(:,i);
   neure_output(:,:,i)=tanh(neure_output(:,:,i)+bias_output(i));
end

tmp_neure_output=reshape(neure_output,10,1);

%%

% 下面是反向传播过程，公式遵循以前的数据


label=zeros(10,1);
label(9)=1;


tmp_delta_neure_output=(tmp_neure_output-label).*(1-tmp_neure_output.^2);
delta_neure_output=reshape(tmp_delta_neure_output,1,1,10);

%%
%全连接层误差
tmp_delta_neure_connect=tmp_delta_neure_output'*w_num_output';
tmp_delta_neure_connect=tmp_delta_neure_connect.*(1-tmp_neure_connect.^2);
delta_neure_connect=reshape(tmp_delta_neure_connect,1,1,84);

%计算output层权值更新以及偏置

for j=1:num_output
    for i=1:num_connect
       delta_w_output(i,j)=tmp_neure_connect(i)*tmp_delta_neure_output(j);
    end
    delta_bias_output(j)=tmp_delta_neure_output(j);
end

%%
%  计算c5层的神经元误差

tmp_delta_neure_c5=tmp_delta_neure_connect*w_num_connect';
tmp_delta_neure_c5=tmp_delta_neure_c5.*(1-tmp_neure_c5.^2);
delta_neure_c5=reshape(tmp_delta_neure_c5,1,1,120);

%  计算全连接层的权值更新和偏置
for j=1:num_connect
    for i=1:num_c5
       delta_w_c5(i,j)=tmp_neure_c5(i)*tmp_delta_neure_connect(j);
    end
    delta_bias_connect(j)=tmp_delta_neure_connect(j);
end

%%
%计算s4层神经元误差

for i=1:num_s4
   for x=1:h_s4 
      for y=1:w_s4
         delta_neure_s4(x,y,i)=reshape(w_num_c5(x,y,i,:),1,120)*tmp_delta_neure_c5'*(1-neure_s4(x,y,i)^2);
         
      end
   end
end

% 计算c5层的权值更新

for j=1:num_c5
   for i=1:num_s4
      for x=1:h_s4
         for y=1:w_s4
            delta_w_c5(x,y,i,j)=neure_s4(x,y,i)*delta_neure_c5(:,:,j); 
         end
      end
   end
   delta_bias_c5(j)=delta_neure_c5(:,:,j);
end



%%
% 计算c3层的神经元误差

for i=1:num_c3
   for x=1:h_c3
      for y = 1:w_c3
          %  由于用了平均权值，所以要乘上1/4  这一点是比较关键的
         delta_neure_c3(x,y,i)=w_num_s4(i)*delta_neure_s4(fix((x+1)/2),fix((y+1)/2),i)*(1-neure_c3(x,y,i)^2)*(1/4);
      end
   end
end

%  计算s4层的权值更新和偏置
%  在计算之前提前把这里的数据归零，防止以前的数据造成影响
delta_w_s4=delta_w_s4*0;
for i=1:num_s4
   for x=1:h_c3
       for y=1:w_c3
          delta_w_s4(i)=delta_w_s4(i)+neure_c3(x,y,i)*delta_neure_s4(fix((x+1)/2),fix((y+1)/2),i);
          
       end
   end
end
delta_w_s4=delta_w_s4/4;
delta_bias_s4=reshape(sum(sum(delta_neure_s4)),1,16);

%%
%  s2层神经元误差
%  记得利用表格确定是否存在连接

delta_neure_s2=delta_neure_s2*0;
for i=1:num_s2
   for j=1:num_c3
      if tablets(i,j)==1
          for x=1:h_c3
             for y=1:w_c3
                delta_neure_s2(x:x+4,y:y+4,i)=delta_neure_s2(x:x+4,y:y+4,i)+delta_neure_c3(x,y,j).*w_num_c3(:,:,i,j);
                
             end
          end
      end
   end
end
delta_neure_s2=delta_neure_s2.*(1-neure_s2.^2);

%  计算c3的权值更新

for j=1:num_c3
   for i=1:num_s2
      for x=1:h_conv
         for y=1:w_conv
            delta_w_c3(x,y,i,j)=sum(sum(delta_neure_c3(:,:,j).*neure_s2(x:x+h_c3-1,y:y+w_c3-1,i)));
            
         end
      end
   end
   delta_bias_c3(j)=sum(sum(delta_neure_c3(:,:,j)));
end

%%
%  c1层
for i=1:num_c1
   for x=1:h_c1
      for y=1:w_c1
         delta_neure_c1(x,y,i)=w_num_s2(i)*delta_neure_s2(fix((x+1)/2),fix((y+1)/2),i)*(1-neure_c1(x,y,i)^2)*(1/4);
         %  这里用到了平均卷积，故乘上1/4
      end
   end
end

%  计算s2的权值更新
delta_w_s2=delta_w_s2*0;
for i=1:num_s2
   for x=1:h_c1
      for y=1:w_c1
          delta_w_s2(i)=delta_w_s2(i)+neure_c1(x,y,i)*delta_neure_s2(fix((x+1)/2),fix((y+1)/2),i);
          
      end
   end
end
delta_w_s2=delta_w_s2/4;
delta_bias_s2=reshape(sum(sum(delta_neure_s2)),1,6);

%%
% 根据输入对c1进行相应的更新
for i=1:num_c1
   for x=1:h_conv
      for y=1:w_conv
         delta_w_c1(x,y,i)=sum(sum(delta_neure_c1(:,:,i).*img(x:x+h_c1-1,y:y+w_c1-1)));
         
      end
   end
   delta_bias_c1(i)=sum(sum(delta_neure_c1(:,:,i)));
   
end

%%

%  下面进行各种权值的更新

w_num_c1=w_num_c1-delta_w_c1*learning_rate;
w_num_s2=w_num_s2-delta_w_s2*learning_rate;
w_num_c3=w_num_c3-delta_w_c3*learning_rate;
w_num_s4=w_num_s4-delta_w_s4*learning_rate;
%  这里的c5权值有一些问题！我去！
%w_num_c5=w_num_c5-delta_w_c5*learning_rate;
w_num_connect=w_num_connect-delta_w_connect*learning_rate;
w_num_output=w_num_output-delta_w_output*learning_rate;

bias_c1=bias_c1-delta_bias_c1*learning_rate;
bias_s2=bias_s2-delta_bias_s2*learning_rate;
bias_c3=bias_c3-delta_bias_c3*learning_rate;
bias_s4=bias_s4-delta_bias_s4*learning_rate;
bias_c5=bias_c5-delta_bias_c5*learning_rate;
bias_connect=bias_connect-delta_bias_connect*learning_rate;
bias_output=bias_output-delta_bias_output*learning_rate;
